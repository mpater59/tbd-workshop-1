{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a0ed7a-a23a-4523-9171-f1d076700168",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_BUCKET=tbd-2024l-303946-data\n",
      "env: GEN_OUTPUT_DIR=/tmp/tpc-di\n",
      "env: REPO_ROOT=/home/jupyter/git/tbd-tpc-di/\n"
     ]
    }
   ],
   "source": [
    "%env DATA_BUCKET=tbd-2024l-303946-data\n",
    "%env GEN_OUTPUT_DIR=/tmp/tpc-di\n",
    "%env REPO_ROOT=/home/jupyter/git/tbd-tpc-di/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fbec0d-d410-4de9-8dfd-00b07fdb7b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typer==0.9.0 (from typer[All]==0.9.0)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting google-cloud-storage==2.13.0\n",
      "  Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (4.12.2)\n",
      "Collecting google-auth<3.0dev,>=2.23.3 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_auth-2.30.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_resumable_media-2.7.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.32.3)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from typer[All]==0.9.0) (0.4.6)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[All]==0.9.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich<14.0.0,>=10.11.0 (from typer[All]==0.9.0)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0)\n",
      "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0) (4.25.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0)\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2019.11.28)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[All]==0.9.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.1-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.2/229.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typer, shellingham, pyasn1, proto-plus, mdurl, googleapis-common-protos, google-crc32c, cachetools, rsa, pyasn1-modules, markdown-it-py, google-resumable-media, rich, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed cachetools-5.3.3 google-api-core-2.19.0 google-auth-2.30.0 google-cloud-core-2.4.1 google-cloud-storage-2.13.0 google-crc32c-1.5.0 google-resumable-media-2.7.1 googleapis-common-protos-1.63.1 markdown-it-py-3.0.0 mdurl-0.1.2 proto-plus-1.23.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 rich-13.7.1 rsa-4.9 shellingham-1.5.4 typer-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3.8 install typer[All]==0.9.0 google-cloud-storage==2.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b5820-852d-4c30-a178-9dd78bbdbd5f",
   "metadata": {},
   "source": [
    "## Install SDKMAN for setting up JVM 8 enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3cdb5b-4978-479e-85c7-c9dbac3e65fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                -+syyyyyyys:\n",
      "                            `/yho:`       -yd.\n",
      "                         `/yh/`             +m.\n",
      "                       .oho.                 hy                          .`\n",
      "                     .sh/`                   :N`                `-/o`  `+dyyo:.\n",
      "                   .yh:`                     `M-          `-/osysoym  :hs` `-+sys:      hhyssssssssy+\n",
      "                 .sh:`                       `N:          ms/-``  yy.yh-      -hy.    `.N-````````+N.\n",
      "               `od/`                         `N-       -/oM-      ddd+`     `sd:     hNNm        -N:\n",
      "              :do`                           .M.       dMMM-     `ms.      /d+`     `NMMs       `do\n",
      "            .yy-                             :N`    ```mMMM.      -      -hy.       /MMM:       yh\n",
      "          `+d+`           `:/oo/`       `-/osyh/ossssssdNMM`           .sh:         yMMN`      /m.\n",
      "         -dh-           :ymNMMMMy  `-/shmNm-`:N/-.``   `.sN            /N-         `NMMy      .m/\n",
      "       `oNs`          -hysosmMMMMydmNmds+-.:ohm           :             sd`        :MMM/      yy\n",
      "      .hN+           /d:    -MMMmhs/-.`   .MMMh   .ss+-                 `yy`       sMMN`     :N.\n",
      "     :mN/           `N/     `o/-`         :MMMo   +MMMN-         .`      `ds       mMMh      do\n",
      "    /NN/            `N+....--:/+oooosooo+:sMMM:   hMMMM:        `my       .m+     -MMM+     :N.\n",
      "   /NMo              -+ooooo+/:-....`...:+hNMN.  `NMMMd`        .MM/       -m:    oMMN.     hs\n",
      "  -NMd`                                    :mm   -MMMm- .s/     -MMm.       /m-   mMMd     -N.\n",
      " `mMM/                                      .-   /MMh. -dMo     -MMMy        od. .MMMs..---yh\n",
      " +MMM.                                           sNo`.sNMM+     :MMMM/        sh`+MMMNmNm+++-\n",
      " mMMM-                                           /--ohmMMM+     :MMMMm.       `hyymmmdddo\n",
      " MMMMh.                  ````                  `-+yy/`yMMM/     :MMMMMy       -sm:.``..-:-.`\n",
      " dMMMMmo-.``````..-:/osyhddddho.           `+shdh+.   hMMM:     :MmMMMM/   ./yy/` `:sys+/+sh/\n",
      " .dMMMMMMmdddddmmNMMMNNNNNMMMMMs           sNdo-      dMMM-  `-/yd/MMMMm-:sy+.   :hs-      /N`\n",
      "  `/ymNNNNNNNmmdys+/::----/dMMm:          +m-         mMMM+ohmo/.` sMMMMdo-    .om:       `sh\n",
      "     `.-----+/.`       `.-+hh/`         `od.          NMMNmds/     `mmy:`     +mMy      `:yy.\n",
      "           /moyso+//+ossso:.           .yy`          `dy+:`         ..       :MMMN+---/oys:\n",
      "         /+m:  `.-:::-`               /d+                                    +MMMMMMMNh:`\n",
      "        +MN/                        -yh.                                     `+hddhy+.\n",
      "       /MM+                       .sh:\n",
      "      :NMo                      -sh/\n",
      "     -NMs                    `/yy:\n",
      "    .NMy                  `:sh+.\n",
      "   `mMm`               ./yds-\n",
      "  `dMMMmyo:-.````.-:oymNy:`\n",
      "  +NMMMMMMMMMMMMMMMMms:`\n",
      "    -+shmNMMMNmdy+:`\n",
      "\n",
      "\n",
      "                                                                 Now attempting installation...\n",
      "\n",
      "\n",
      "Looking for a previous installation of SDKMAN...\n",
      "Looking for unzip...\n",
      "Looking for zip...\n",
      "Looking for curl...\n",
      "Looking for sed...\n",
      "Installing SDKMAN scripts...\n",
      "Create distribution directories...\n",
      "Getting available candidates...\n",
      "Prime platform file...\n",
      "Prime the config file...\n",
      "Installing script cli archive...\n",
      "* Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Checking archive integrity...\n",
      "* Extracting archive...\n",
      "* Copying archive contents...\n",
      "* Cleaning up...\n",
      "\n",
      "Installing script cli archive...\n",
      "* Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Checking archive integrity...\n",
      "* Extracting archive...\n",
      "* Copying archive contents...\n",
      "* Cleaning up...\n",
      "\n",
      "Set version to 5.18.2 ...\n",
      "Set native version to 0.4.6 ...\n",
      "Attempt update of interactive bash profile on regular UNIX...\n",
      "Added sdkman init snippet to /root/.bashrc\n",
      "Attempt update of zsh profile...\n",
      "Updated existing /root/.zshrc\n",
      "\n",
      "\n",
      "\n",
      "All done!\n",
      "\n",
      "\n",
      "You are subscribed to the STABLE channel.\n",
      "\n",
      "Please open a new terminal, or run the following in the existing one:\n",
      "\n",
      "    source \"/root/.sdkman/bin/sdkman-init.sh\"\n",
      "\n",
      "Then issue the following command:\n",
      "\n",
      "    sdk help\n",
      "\n",
      "Enjoy!!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s https://get.sdkman.io | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691084f8-a883-408a-8d16-cc717669c04d",
   "metadata": {},
   "source": [
    "## Install and set as default JVM 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275488e6-68e3-4f42-a3ca-060b286bade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: java 8.0.392-amzn\n",
      "\n",
      "In progress...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repackaging Java 8.0.392-amzn...\n",
      "\n",
      "Done repackaging...\n",
      "\n",
      "\u001b[1;32mInstalling: java 8.0.392-amzn\u001b[0m\n",
      "\u001b[1;32mDone installing!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;32mSetting java 8.0.392-amzn as default.\u001b[0m\n",
      "\n",
      "\u001b[1;32mUsing java version 8.0.392-amzn in this shell.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 8.0.392-amzn\n",
    "sdk use java 8.0.392-amzn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554db58-b832-45e2-a1f6-28bd873ae31d",
   "metadata": {},
   "source": [
    "## Check if JVM 8 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898835b7-c347-49dd-bc0f-ca845375e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_392\"\n",
      "OpenJDK Runtime Environment Corretto-8.392.08.1 (build 1.8.0_392-b08)\n",
      "OpenJDK 64-Bit Server VM Corretto-8.392.08.1 (build 25.392-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45d9a3-e599-4661-944b-1bdfe3808c19",
   "metadata": {},
   "source": [
    "## Clone tbd-tpc-di repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7481d445-4ea6-40cc-8320-5520fe8d4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tbd-tpc-di'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'notebook'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'notebook' set up to track remote branch 'notebook' from 'origin'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p /home/jupyter/git && cd /home/jupyter/git\n",
    "git clone https://github.com/mpater59/tbd-tpc-di.git\n",
    "cd tbd-tpc-di\n",
    "git pull\n",
    "git checkout notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b992bfc-8f8c-42eb-bf07-069b0d897fda",
   "metadata": {},
   "source": [
    "## Generate input dataset (run this cell below from the terminal!!!)\n",
    "It should take approx. 15min with scale factor set to 100 and generate approx. 10GiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ab901-15dc-4e94-b4fd-e2b9cfc6e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd /home/jupyter/git/tbd-tpc-di/tools/ \n",
    "java -jar DIGen.jar -sf 100 -o /tmp/tpc-di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a24c0aa-bff7-4831-869d-3183e7373a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup JVM 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e466e8-41aa-4afc-8392-5675d36adcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: java 11.0.21-amzn\n",
      "\n",
      "In progress...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repackaging Java 11.0.21-amzn...\n",
      "\n",
      "Done repackaging...\n",
      "\n",
      "\u001b[1;32mInstalling: java 11.0.21-amzn\u001b[0m\n",
      "\u001b[1;32mDone installing!\u001b[0m\n",
      "\n",
      "\u001b[1;33mDo you want java 11.0.21-amzn to be set as default? (Y/n): \u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 11.0.21-amzn\n",
    "sdk use java 11.0.21-amzn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccda4c-40d4-40fe-9d2e-68b416522f64",
   "metadata": {},
   "source": [
    "## Load staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259b72bc-2efd-405b-883f-618a9772bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-42976f18-05fb-4cfe-961c-d3b851de184d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 982ms :: artifacts dl 42ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-42976f18-05fb-4cfe-961c-d3b851de184d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/13ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 13:17:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 13:17:34 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/06/11 13:17:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/06/11 13:17:43 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:17:43 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:17:43 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/06/11 13:17:43 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:17:43 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:18:11 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "24/06/11 13:23:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "DATE table created.\n",
      "DAILY_MARKET table created.\n",
      "INDUSTRY table created.\n",
      "PROSPECT table created.\n",
      "CUSTOMER_MGMT table created.\n",
      "TAX_RATE table created.\n",
      "HR table created.\n",
      "WATCH_HISTORY table created.\n",
      "TRADE table created.\n",
      "TRADE_HISTORY table created.\n",
      "STATUS_TYPE table created.\n",
      "TRADE_TYPE table created.\n",
      "HOLDING_HISTORY table created.\n",
      "CASH_TRANSACTION table created.\n",
      "CMP table created.\n",
      "SEC table created.\n",
      "FIN table created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd $REPO_ROOT\n",
    "python3.8 tpcdi.py --output-directory $GEN_OUTPUT_DIR --stage $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0740193-5da4-4aac-9349-aa4e76f4e99b",
   "metadata": {},
   "source": [
    "## Run dbt ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d0642c-b5c9-4aa8-996f-40f82e3c90e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m13:39:43  Running with dbt=1.7.13\n",
      "\u001b[0m13:39:44  Updating lock file in file path: /home/jupyter/git/tbd-tpc-di/package-lock.yml\n",
      "\u001b[0m13:39:44  Installing dbt-labs/dbt_utils\n",
      "\u001b[0m13:39:44  Installed from version 1.1.1\n",
      "\u001b[0m13:39:44  Updated version available: 1.2.0\n",
      "\u001b[0m13:39:44  \n",
      "\u001b[0m13:39:44  Updates available for packages: ['dbt-labs/dbt_utils']                 \n",
      "Update your versions in packages.yml, then run dbt deps\n",
      "\u001b[0m13:39:49  Running with dbt=1.7.13\n",
      "\u001b[0m13:39:50  Registered adapter: spark=1.7.1\n",
      "\u001b[0m13:39:50  Unable to do partial parsing because saved manifest not found. Starting full parse.\n",
      "\u001b[0m13:39:55  Found 44 models, 1 test, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m13:39:55  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-26223587-a4b8-43f5-88a0-d94f235a08ee;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 1430ms :: artifacts dl 63ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-26223587-a4b8-43f5-88a0-d94f235a08ee\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/53ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 13:40:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 13:40:08 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/06/11 13:40:09 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/06/11 13:40:17 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:40:17 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:40:17 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/06/11 13:40:17 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:40:17 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/06/11 13:40:45 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m13:40:48  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m13:40:48  \n",
      "\u001b[0m13:40:48  1 of 43 START sql table model demo_bronze.brokerage_cash_transaction ........... [RUN]\n",
      "24/06/11 13:40:49 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/06/11 13:40:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m13:41:32  1 of 43 OK created sql table model demo_bronze.brokerage_cash_transaction ...... [\u001b[32mOK\u001b[0m in 43.92s]\n",
      "\u001b[0m13:41:32  2 of 43 START sql table model demo_bronze.brokerage_daily_market ............... [RUN]\n",
      "24/06/11 13:41:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:43:06  2 of 43 OK created sql table model demo_bronze.brokerage_daily_market .......... [\u001b[32mOK\u001b[0m in 94.41s]\n",
      "\u001b[0m13:43:06  3 of 43 START sql table model demo_bronze.brokerage_holding_history ............ [RUN]\n",
      "24/06/11 13:43:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:43:20  3 of 43 OK created sql table model demo_bronze.brokerage_holding_history ....... [\u001b[32mOK\u001b[0m in 13.57s]\n",
      "\u001b[0m13:43:20  4 of 43 START sql table model demo_bronze.brokerage_trade ...................... [RUN]\n",
      "24/06/11 13:43:20 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:44:15  4 of 43 OK created sql table model demo_bronze.brokerage_trade ................. [\u001b[32mOK\u001b[0m in 55.71s]\n",
      "\u001b[0m13:44:15  5 of 43 START sql table model demo_bronze.brokerage_trade_history .............. [RUN]\n",
      "24/06/11 13:44:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:45:04  5 of 43 OK created sql table model demo_bronze.brokerage_trade_history ......... [\u001b[32mOK\u001b[0m in 48.72s]\n",
      "\u001b[0m13:45:04  6 of 43 START sql table model demo_bronze.brokerage_watch_history .............. [RUN]\n",
      "24/06/11 13:45:04 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:45:58  6 of 43 OK created sql table model demo_bronze.brokerage_watch_history ......... [\u001b[32mOK\u001b[0m in 53.75s]\n",
      "\u001b[0m13:45:58  7 of 43 START sql table model demo_bronze.crm_customer_mgmt .................... [RUN]\n",
      "24/06/11 13:45:58 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/06/11 13:45:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "\u001b[0m13:46:08  7 of 43 OK created sql table model demo_bronze.crm_customer_mgmt ............... [\u001b[32mOK\u001b[0m in 10.30s]\n",
      "\u001b[0m13:46:08  8 of 43 START sql table model demo_bronze.finwire_company ...................... [RUN]\n",
      "24/06/11 13:46:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:46:11  8 of 43 OK created sql table model demo_bronze.finwire_company ................. [\u001b[32mOK\u001b[0m in 2.60s]\n",
      "\u001b[0m13:46:11  9 of 43 START sql table model demo_bronze.finwire_financial .................... [RUN]\n",
      "24/06/11 13:46:11 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:08  9 of 43 OK created sql table model demo_bronze.finwire_financial ............... [\u001b[32mOK\u001b[0m in 57.26s]\n",
      "\u001b[0m13:47:08  10 of 43 START sql table model demo_bronze.finwire_security .................... [RUN]\n",
      "24/06/11 13:47:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:12  10 of 43 OK created sql table model demo_bronze.finwire_security ............... [\u001b[32mOK\u001b[0m in 3.53s]\n",
      "\u001b[0m13:47:12  11 of 43 START sql table model demo_bronze.hr_employee ......................... [RUN]\n",
      "24/06/11 13:47:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:15  11 of 43 OK created sql table model demo_bronze.hr_employee .................... [\u001b[32mOK\u001b[0m in 3.45s]\n",
      "\u001b[0m13:47:15  12 of 43 START sql table model demo_bronze.reference_date ...................... [RUN]\n",
      "24/06/11 13:47:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:16  12 of 43 OK created sql table model demo_bronze.reference_date ................. [\u001b[32mOK\u001b[0m in 1.25s]\n",
      "\u001b[0m13:47:16  13 of 43 START sql table model demo_bronze.reference_industry .................. [RUN]\n",
      "24/06/11 13:47:16 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:17  13 of 43 OK created sql table model demo_bronze.reference_industry ............. [\u001b[32mOK\u001b[0m in 0.92s]\n",
      "\u001b[0m13:47:17  14 of 43 START sql table model demo_bronze.reference_status_type ............... [RUN]\n",
      "24/06/11 13:47:17 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:18  14 of 43 OK created sql table model demo_bronze.reference_status_type .......... [\u001b[32mOK\u001b[0m in 1.12s]\n",
      "\u001b[0m13:47:18  15 of 43 START sql table model demo_bronze.reference_tax_rate .................. [RUN]\n",
      "24/06/11 13:47:19 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:19  15 of 43 OK created sql table model demo_bronze.reference_tax_rate ............. [\u001b[32mOK\u001b[0m in 0.97s]\n",
      "\u001b[0m13:47:19  16 of 43 START sql table model demo_bronze.reference_trade_type ................ [RUN]\n",
      "24/06/11 13:47:20 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:20  16 of 43 OK created sql table model demo_bronze.reference_trade_type ........... [\u001b[32mOK\u001b[0m in 1.10s]\n",
      "\u001b[0m13:47:20  17 of 43 START sql table model demo_bronze.syndicated_prospect ................. [RUN]\n",
      "24/06/11 13:47:21 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:47:28  17 of 43 OK created sql table model demo_bronze.syndicated_prospect ............ [\u001b[32mOK\u001b[0m in 7.38s]\n",
      "\u001b[0m13:47:28  18 of 43 START sql table model demo_silver.daily_market ........................ [RUN]\n",
      "24/06/11 13:47:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:07:11  18 of 43 OK created sql table model demo_silver.daily_market ................... [\u001b[32mOK\u001b[0m in 1183.64s]\n",
      "\u001b[0m14:07:12  19 of 43 START sql table model demo_silver.employees ........................... [RUN]\n",
      "24/06/11 14:07:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:07:16  19 of 43 OK created sql table model demo_silver.employees ...................... [\u001b[32mOK\u001b[0m in 4.49s]\n",
      "\u001b[0m14:07:16  20 of 43 START sql table model demo_silver.date ................................ [RUN]\n",
      "24/06/11 14:07:16 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:07:18  20 of 43 OK created sql table model demo_silver.date ........................... [\u001b[32mOK\u001b[0m in 2.09s]\n",
      "\u001b[0m14:07:18  21 of 43 START sql table model demo_silver.companies ........................... [RUN]\n",
      "24/06/11 14:07:19 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:07:24  21 of 43 OK created sql table model demo_silver.companies ...................... [\u001b[32mOK\u001b[0m in 6.18s]\n",
      "\u001b[0m14:07:24  22 of 43 START sql table model demo_silver.accounts ............................ [RUN]\n",
      "24/06/11 14:07:25 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:07:41  22 of 43 OK created sql table model demo_silver.accounts ....................... [\u001b[32mOK\u001b[0m in 13.34s]\n",
      "\u001b[0m14:07:41  23 of 43 START sql table model demo_silver.customers ........................... [RUN]\n",
      "24/06/11 14:07:41 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:07:50  23 of 43 OK created sql table model demo_silver.customers ...................... [\u001b[32mOK\u001b[0m in 9.66s]\n",
      "\u001b[0m14:07:50  24 of 43 START sql table model demo_silver.trades_history ...................... [RUN]\n",
      "24/06/11 14:07:51 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:13:00  24 of 43 OK created sql table model demo_silver.trades_history ................. [\u001b[32mOK\u001b[0m in 309.41s]\n",
      "\u001b[0m14:13:00  25 of 43 START sql table model demo_gold.dim_broker ............................ [RUN]\n",
      "24/06/11 14:13:00 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:13:04  25 of 43 OK created sql table model demo_gold.dim_broker ....................... [\u001b[32mOK\u001b[0m in 4.45s]\n",
      "\u001b[0m14:13:04  26 of 43 START sql table model demo_gold.dim_date .............................. [RUN]\n",
      "24/06/11 14:13:04 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:13:06  26 of 43 OK created sql table model demo_gold.dim_date ......................... [\u001b[32mOK\u001b[0m in 1.49s]\n",
      "\u001b[0m14:13:06  27 of 43 START sql table model demo_gold.dim_company ........................... [RUN]\n",
      "24/06/11 14:13:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:13:09  27 of 43 OK created sql table model demo_gold.dim_company ...................... [\u001b[32mOK\u001b[0m in 3.30s]\n",
      "\u001b[0m14:13:09  28 of 43 START sql table model demo_silver.financials .......................... [RUN]\n",
      "24/06/11 14:13:09 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:18  28 of 43 OK created sql table model demo_silver.financials ..................... [\u001b[32mOK\u001b[0m in 68.55s]\n",
      "\u001b[0m14:14:18  29 of 43 START sql table model demo_silver.securities .......................... [RUN]\n",
      "24/06/11 14:14:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:24  29 of 43 OK created sql table model demo_silver.securities ..................... [\u001b[32mOK\u001b[0m in 6.47s]\n",
      "\u001b[0m14:14:24  30 of 43 START sql table model demo_silver.cash_transactions ................... [RUN]\n",
      "24/06/11 14:14:24 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:15:14  30 of 43 OK created sql table model demo_silver.cash_transactions .............. [\u001b[32mOK\u001b[0m in 49.54s]\n",
      "\u001b[0m14:15:14  31 of 43 START sql table model demo_gold.dim_customer .......................... [RUN]\n",
      "24/06/11 14:15:14 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:15:30  31 of 43 OK created sql table model demo_gold.dim_customer ..................... [\u001b[32mOK\u001b[0m in 16.89s]\n",
      "\u001b[0m14:15:31  32 of 43 START sql table model demo_gold.dim_trade ............................. [RUN]\n",
      "24/06/11 14:15:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:18:47  32 of 43 OK created sql table model demo_gold.dim_trade ........................ [\u001b[32mOK\u001b[0m in 196.88s]\n",
      "\u001b[0m14:18:47  33 of 43 START sql table model demo_silver.trades .............................. [RUN]\n",
      "24/06/11 14:18:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:22:28  33 of 43 OK created sql table model demo_silver.trades ......................... [\u001b[32mOK\u001b[0m in 220.63s]\n",
      "\u001b[0m14:22:28  34 of 43 START sql table model demo_gold.dim_security .......................... [RUN]\n",
      "24/06/11 14:22:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:22:32  34 of 43 OK created sql table model demo_gold.dim_security ..................... [\u001b[32mOK\u001b[0m in 3.89s]\n",
      "\u001b[0m14:22:32  35 of 43 START sql table model demo_silver.watches_history ..................... [RUN]\n",
      "24/06/11 14:22:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:24:18  35 of 43 OK created sql table model demo_silver.watches_history ................ [\u001b[32mOK\u001b[0m in 105.83s]\n",
      "\u001b[0m14:24:18  36 of 43 START sql table model demo_gold.dim_account ........................... [RUN]\n",
      "24/06/11 14:24:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:24:29  36 of 43 OK created sql table model demo_gold.dim_account ...................... [\u001b[32mOK\u001b[0m in 10.73s]\n",
      "\u001b[0m14:24:29  37 of 43 START sql table model demo_silver.holdings_history .................... [RUN]\n",
      "24/06/11 14:24:29 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:26:02  37 of 43 OK created sql table model demo_silver.holdings_history ............... [\u001b[32mOK\u001b[0m in 93.08s]\n",
      "\u001b[0m14:26:02  38 of 43 START sql table model demo_silver.watches ............................. [RUN]\n",
      "24/06/11 14:26:02 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:28:39  38 of 43 OK created sql table model demo_silver.watches ........................ [\u001b[32mOK\u001b[0m in 157.34s]\n",
      "\u001b[0m14:28:39  39 of 43 START sql table model demo_gold.fact_cash_transactions ................ [RUN]\n",
      "24/06/11 14:28:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:30:32  39 of 43 OK created sql table model demo_gold.fact_cash_transactions ........... [\u001b[32mOK\u001b[0m in 113.11s]\n",
      "\u001b[0m14:30:32  40 of 43 START sql table model demo_gold.fact_trade ............................ [RUN]\n",
      "24/06/11 14:30:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:39:14  40 of 43 OK created sql table model demo_gold.fact_trade ....................... [\u001b[32mOK\u001b[0m in 521.74s]\n",
      "\u001b[0m14:39:14  41 of 43 START sql table model demo_gold.fact_holdings ......................... [RUN]\n",
      "24/06/11 14:39:14 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:56:50  41 of 43 OK created sql table model demo_gold.fact_holdings .................... [\u001b[32mOK\u001b[0m in 1056.21s]\n",
      "\u001b[0m14:56:50  42 of 43 START sql table model demo_gold.fact_watches .......................... [RUN]\n",
      "24/06/11 14:56:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m15:00:57  42 of 43 OK created sql table model demo_gold.fact_watches ..................... [\u001b[32mOK\u001b[0m in 246.61s]\n",
      "\u001b[0m15:00:57  43 of 43 START sql table model demo_gold.fact_cash_balances .................... [RUN]\n",
      "24/06/11 15:00:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m15:04:08  43 of 43 OK created sql table model demo_gold.fact_cash_balances ............... [\u001b[32mOK\u001b[0m in 191.53s]\n",
      "\u001b[0m15:04:08  \n",
      "\u001b[0m15:04:08  Finished running 43 table models in 1 hours 24 minutes and 13.28 seconds (5053.28s).\n",
      "\u001b[0m15:04:08  \n",
      "\u001b[0m15:04:08  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m15:04:08  \n",
      "\u001b[0m15:04:08  Done. PASS=43 WARN=0 ERROR=0 SKIP=0 TOTAL=43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-39' coro=<ScriptMagics.shebang.<locals>._handle_stream() done, defined at /usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py:211> exception=ValueError('Separator is not found, and chunk exceed the limit')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 540, in readline\n",
      "    line = await self.readuntil(sep)\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 618, in readuntil\n",
      "    raise exceptions.LimitOverrunError(\n",
      "asyncio.exceptions.LimitOverrunError: Separator is not found, and chunk exceed the limit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\", line 213, in _handle_stream\n",
      "    line = (await stream.readline()).decode(\"utf8\", errors=\"replace\")\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 549, in readline\n",
      "    raise ValueError(e.args[0])\n",
      "ValueError: Separator is not found, and chunk exceed the limit\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt deps\n",
    "dbt run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae93304-68eb-46c8-86a3-26feec06a54a",
   "metadata": {},
   "source": [
    "## Run dbt tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6b563f4-0b31-40a3-b353-9e6c9de8c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m16:24:10  Running with dbt=1.7.13\n",
      "\u001b[0m16:24:11  Registered adapter: spark=1.7.1\n",
      "\u001b[0m16:24:12  Found 44 models, 4 tests, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m16:24:12  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0bdedbf5-828e-42f4-ad7c-755e95113d26;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 573ms :: artifacts dl 48ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0bdedbf5-828e-42f4-ad7c-755e95113d26\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/32ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 16:24:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 16:24:21 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/06/11 16:24:21 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/06/11 16:24:29 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/06/11 16:24:29 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/06/11 16:24:29 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/06/11 16:24:29 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/06/11 16:24:29 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/06/11 16:24:56 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m16:24:59  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m16:24:59  \n",
      "\u001b[0m16:24:59  1 of 4 START test dim_customer__unique_customer ................................ [RUN]\n",
      "24/06/11 16:25:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/06/11 16:25:01 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m16:25:16  1 of 4 PASS dim_customer__unique_customer ...................................... [\u001b[32mPASS\u001b[0m in 17.17s]\n",
      "\u001b[0m16:25:16  2 of 4 START test fact_cash_transactions__null_amount .......................... [RUN]\n",
      "\u001b[0m16:25:34  2 of 4 PASS fact_cash_transactions__null_amount ................................ [\u001b[32mPASS\u001b[0m in 17.18s]\n",
      "\u001b[0m16:25:34  3 of 4 START test fact_cash_transactions__unique_cash_transaction .............. [RUN]\n",
      "\u001b[0m16:26:10  3 of 4 PASS fact_cash_transactions__unique_cash_transaction .................... [\u001b[32mPASS\u001b[0m in 36.04s]\n",
      "\u001b[0m16:26:10  4 of 4 START test fact_trade__unique_trade ..................................... [RUN]\n",
      "\u001b[0m16:26:26  4 of 4 PASS fact_trade__unique_trade ........................................... [\u001b[32mPASS\u001b[0m in 16.47s]\n",
      "\u001b[0m16:26:26  \n",
      "\u001b[0m16:26:26  Finished running 4 tests in 0 hours 2 minutes and 14.62 seconds (134.62s).\n",
      "\u001b[0m16:26:26  \n",
      "\u001b[0m16:26:26  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m16:26:26  \n",
      "\u001b[0m16:26:26  Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6ce00fe-ce46-4202-a798-a7f227ff3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 15:34:15 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/06/11 15:34:22 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/06/11 15:34:22 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/06/11 15:34:22 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/06/11 15:34:22 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/06/11 15:34:22 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TBD-TPC-DI-setup\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c892605-9496-4526-b574-a19168678934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/11 15:15:00 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|     bronze|\n",
      "|    default|\n",
      "|demo_bronze|\n",
      "|  demo_gold|\n",
      "|demo_silver|\n",
      "|      digen|\n",
      "|       gold|\n",
      "|     silver|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2e94c83-4983-49f5-8597-77e3b9c4661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use demo_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44296edd-82e6-4600-b730-d2cc4992a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|demo_gold|         dim_account|      false|\n",
      "|demo_gold|          dim_broker|      false|\n",
      "|demo_gold|         dim_company|      false|\n",
      "|demo_gold|        dim_customer|      false|\n",
      "|demo_gold|            dim_date|      false|\n",
      "|demo_gold|        dim_security|      false|\n",
      "|demo_gold|           dim_trade|      false|\n",
      "|demo_gold|  fact_cash_balances|      false|\n",
      "|demo_gold|fact_cash_transac...|      false|\n",
      "|demo_gold|       fact_holdings|      false|\n",
      "|demo_gold|          fact_trade|      false|\n",
      "|demo_gold|        fact_watches|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b016b0df-fbec-470c-86f3-5844a2477acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:===============================================>        (16 + 3) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+---------------------+---------+--------------------+\n",
      "|      sk_customer_id|       sk_account_id|sk_transaction_date|transaction_timestamp|   amount|         description|\n",
      "+--------------------+--------------------+-------------------+---------------------+---------+--------------------+\n",
      "|98e74c5041e841187...|e2fbe67d273947888...|         2012-07-10|  2012-07-10 20:14:28|-78741.52|JLsYXheNdilBBEVsd...|\n",
      "|98e74c5041e841187...|e2fbe67d273947888...|         2012-09-30|  2012-09-30 00:03:22| -4852.15|BYEt AR OBLRJccDL...|\n",
      "|98e74c5041e841187...|e2fbe67d273947888...|         2012-07-14|  2012-07-14 18:39:27| -7604.54|ErTULFSXRWLMNYnwi...|\n",
      "|98e74c5041e841187...|e2fbe67d273947888...|         2012-09-24|  2012-09-24 14:43:30| -5055.03|sCrBTtBQcYEtFHVPr...|\n",
      "|98e74c5041e841187...|e2fbe67d273947888...|         2012-07-12|  2012-07-12 00:35:17| -5777.79|MvgsIWDqSHvElkMAR...|\n",
      "+--------------------+--------------------+-------------------+---------------------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from fact_cash_transactions limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4fdacd4a-63de-411b-937f-f0479ad934b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sk_customer_id',\n",
       " 'sk_account_id',\n",
       " 'sk_transaction_date',\n",
       " 'transaction_timestamp',\n",
       " 'amount',\n",
       " 'description']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from fact_cash_transactions limit 5\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9ba6b8e-c1b3-48af-813b-0adbef8121e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer bronze - Number of tables: 0\n",
      "Layer default - Number of tables: 0\n",
      "Layer demo_bronze - Number of tables: 17\n",
      "Layer demo_gold - Number of tables: 12\n",
      "Layer demo_silver - Number of tables: 14\n",
      "Layer digen - Number of tables: 17\n",
      "Layer gold - Number of tables: 0\n",
      "Layer silver - Number of tables: 0\n"
     ]
    }
   ],
   "source": [
    "databases = []\n",
    "result = {}\n",
    "for value in spark.sql(\"show databases\").collect():\n",
    "    databases.append(value.namespace)\n",
    "\n",
    "for database in databases:\n",
    "    spark.sql(f\"use {database}\")\n",
    "    tables = spark.sql(\"show tables\")\n",
    "    result[database] = tables.count()\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"Layer {key} - Number of tables: {value}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e34fa44-084c-4445-ad41-17b4762a0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190b0c0-b4e2-4aed-85aa-1b78b77d1c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
